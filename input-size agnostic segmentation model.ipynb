{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ef77fe-6e2a-4e3d-abb0-bcb67211d3cd",
   "metadata": {},
   "source": [
    "# Model independent of input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff75746-6df4-4cae-a7ca-3a0582a33b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696ab164-c57e-4a28-990c-f5c90eeb3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad6386-6654-4826-9a9d-a75cfec5b42a",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "- Most segmentation models that follows the encoder-decoder paradigm are input-size independent, because the input and output are the same. The only constraint is the input size should be large enough to survive the downsample part of the network.\n",
    "- Some segmentation model have extra constraints. For example, DeepLabv3 needs that H and W should be multiples of 8 or 16, depending on the selected stride. Otherwise, rounding during downsampling/upsampling can cause tiny misalignments.\n",
    "- Models based on early architectures, e.g., Alexnet-based where FC layers are repurposed for segmentation. But, if theses layers are replaced by 1x1 convolutions, the model can be input-size independent.\n",
    "\n",
    "The important takeaways is that most models are independent or can be somehow adapted to be independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ece76e4-3c13-4f92-acab-2573e57a56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super().__init__()\n",
    "        # Load EfficientNet backbone (no classifier head)\n",
    "        # we will use it as the encoder\n",
    "        backbone = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        self.encoder = backbone.features  # (b, 1280, h/32, w/32)\n",
    "\n",
    "        # Simple decoder (upsample back to input size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),  # h/16, w/16\n",
    "\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),  # h/8, w/8\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),  # h/4, w/4\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=4, mode=\"bilinear\", align_corners=False),  # back to h, w\n",
    "\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)  # final segmentation map\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)  # (b, 1280, h/32, w/32)\n",
    "        out = self.decoder(feats) # (b, num_classes, h, w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2086fc5e-c229-4ee0-92ba-7793863a8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy inputs with different spatial sizes\n",
    "x1 = torch.randn(2, 3, 128, 128).to(device)   # small square\n",
    "x2 = torch.randn(2, 3, 256, 512).to(device)   # rectangular\n",
    "x3 = torch.randn(2, 3, 480, 640).to(device)   # bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2910977e-36f3-4763-b166-79f7a41f726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "[2, 3, 128, 128] -> [2, 16, 128, 128]\n",
      "[2, 3, 256, 512] -> [2, 16, 256, 512]\n",
      "[2, 3, 480, 640] -> [2, 16, 480, 640]\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetSegmentation(num_classes=16)\n",
    "\n",
    "# Forward pass\n",
    "y1 = model(x1)\n",
    "y2 = model(x2)\n",
    "y3 = model(x3)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(list(x1.shape), \"->\", list(y1.shape))  # (128, 128)\n",
    "print(list(x2.shape), \"->\", list(y2.shape))  # (256, 512) - notice that the dimension don't need to be simetrical\n",
    "print(list(x3.shape), \"->\", list(y3.shape))  # (480, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d2236-1c5a-4981-860b-240b11f18b2e",
   "metadata": {},
   "source": [
    "### First version of AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5d8ea9-c011-497e-acae-58d6f3bbb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetSeg(nn.Module):\n",
    "    \"\"\" use 1x1 kernels\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=21):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # load model\n",
    "        alexnet = models.alexnet(weights=None)\n",
    "        \n",
    "        # Use features (convolutions)\n",
    "        self.features = alexnet.features   # -> (b, 256, 6, 6) for 227x227 input\n",
    "        \n",
    "        # But if we convert the classifier FC layers into 1x1 convolutions\n",
    "        # the model works\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256, 4096, kernel_size=1),   # replaces first FC\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, 4096, kernel_size=1),  # replaces second FC\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, num_classes, kernel_size=1)  # final segmentation logits\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.features(x)   # (b, 256, h/32, w/32)\n",
    "        out = self.classifier(feats)  # (b, num_classes, h_out, w_out)\n",
    "        \n",
    "        # Upsample back to input size\n",
    "        out = F.interpolate(out, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6471334c-80f3-47ea-9211-acd04acf230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "[2, 3, 128, 128] -> [2, 21, 128, 128]\n",
      "[2, 3, 256, 512] -> [2, 21, 256, 512]\n",
      "[2, 3, 480, 640] -> [2, 21, 480, 640]\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetSeg(num_classes=21)\n",
    "\n",
    "# Forward pass\n",
    "y1 = model(x1)\n",
    "y2 = model(x2)\n",
    "y3 = model(x3)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(list(x1.shape), \"->\", list(y1.shape))  # (128, 128)\n",
    "print(list(x2.shape), \"->\", list(y2.shape))  # (256, 512) - notice that the dimension don't need to be simetrical\n",
    "print(list(x3.shape), \"->\", list(y3.shape))  # (480, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e641623-22c1-4c7f-98ca-ceb57d7cb5ec",
   "metadata": {},
   "source": [
    "### Second version of AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34bc65e-2bf2-40ca-82ca-dca069f25fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetSeg2(nn.Module):\n",
    "    \"\"\" another way to solve the input-size problem: use AdaptiveAvgPool2d\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=21):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # load model\n",
    "        alexnet = models.alexnet(weights=None)\n",
    "        \n",
    "        # Use features (convolutions)\n",
    "        self.features = alexnet.features   # -> (b, 256, 6, 6) for 227x227 input\n",
    "        \n",
    "        # But if we convert the classifier FC layers into 1x1 convolutions\n",
    "        # the model works\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((6,6)),           # force 6x6 feature map\n",
    "            nn.Conv2d(256, 4096, kernel_size=6),   # replaces first FC\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, 4096, kernel_size=1),  # replaces second FC\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, num_classes, kernel_size=1)  # final segmentation logits\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.features(x)   # (b, 256, h/32, w/32)\n",
    "        out = self.classifier(feats)  # (b, num_classes, h_out, w_out)\n",
    "        \n",
    "        # Upsample back to input size\n",
    "        out = F.interpolate(out, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f72d340-19b9-4acb-8780-db992c3ac5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "[2, 3, 128, 128] -> [2, 21, 128, 128]\n",
      "[2, 3, 256, 512] -> [2, 21, 256, 512]\n",
      "[2, 3, 480, 640] -> [2, 21, 480, 640]\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetSeg2(num_classes=21)\n",
    "\n",
    "# Forward pass\n",
    "y1 = model(x1)\n",
    "y2 = model(x2)\n",
    "y3 = model(x3)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(list(x1.shape), \"->\", list(y1.shape))  # (128, 128)\n",
    "print(list(x2.shape), \"->\", list(y2.shape))  # (256, 512) - notice that the dimension don't need to be simetrical\n",
    "print(list(x3.shape), \"->\", list(y3.shape))  # (480, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d127b-1c97-4c2c-bc6f-000bc03cb5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
